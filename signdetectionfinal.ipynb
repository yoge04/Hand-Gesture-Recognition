{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abb7f3ec-9aac-4bf6-b83c-a8a3eb89fdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use ipykernal\n",
    "#!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e64bb545-09e2-4608-b1c0-ab2ee3209331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "\n",
    "# Gesture Recognition class\n",
    "class GestureRecognition:\n",
    "    def __init__(self):\n",
    "        self.hands = mp.solutions.hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7, min_tracking_confidence=0.5)\n",
    "        self.current_gesture = None\n",
    "\n",
    "    def detect_gesture(self, image):\n",
    "        results = self.hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        if results.multi_hand_landmarks:\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]\n",
    "            self.current_gesture = self.get_gesture(hand_landmarks)\n",
    "            # Draw landmarks on the image for visualization (optional)\n",
    "            mp.solutions.drawing_utils.draw_landmarks(image, hand_landmarks, mp.solutions.hands.HAND_CONNECTIONS)\n",
    "\n",
    "    def get_gesture(self, hand_landmarks):\n",
    "        # Get fingertip positions\n",
    "        thumb_tip = hand_landmarks.landmark[4]\n",
    "        index_finger_tip = hand_landmarks.landmark[8]\n",
    "        middle_finger_tip = hand_landmarks.landmark[12]\n",
    "        ring_finger_tip = hand_landmarks.landmark[16]\n",
    "        little_finger_tip = hand_landmarks.landmark[20]\n",
    "\n",
    "        # Define tips and bases for fingers\n",
    "        tips = [8, 12, 16, 20]  # Index, Middle, Ring, Pinky tips\n",
    "        bases = [5, 9, 13, 17]  # Respective knuckles\n",
    "\n",
    "        # Check for \"Open Palm\" gesture\n",
    "        if all(hand_landmarks.landmark[tip].y < hand_landmarks.landmark[base].y for tip, base in zip(tips, bases)):\n",
    "            return \"Hello\"\n",
    "\n",
    "        # Check for \"Okay\" gesture\n",
    "        if thumb_tip.y < index_finger_tip.y < middle_finger_tip.y < ring_finger_tip.y < little_finger_tip.y:\n",
    "            return \"Okay\"\n",
    "\n",
    "        # Check for \"Dislike\" gesture\n",
    "        elif thumb_tip.y > index_finger_tip.y > middle_finger_tip.y > ring_finger_tip.y > little_finger_tip.y:\n",
    "            return \"Dislike\"\n",
    "        \n",
    "\n",
    "        # Check for \"Point\" gesture (index finger extended and pointing)\n",
    "        elif index_finger_tip.y < middle_finger_tip.y and abs(index_finger_tip.x - middle_finger_tip.x) < 0.2:\n",
    "            return \"Point\"\n",
    "\n",
    "        # Check for \"Victory\" gesture (index and middle fingers up)\n",
    "        elif index_finger_tip.y < thumb_tip.y and middle_finger_tip.y < thumb_tip.y:\n",
    "            return \"Victory\"\n",
    "        \n",
    "\n",
    "        # Check for \"Stop\" gesture (at least 4 fingers extended)\n",
    "        # We will check if the tip of each finger is above the knuckle (indicating it's extended)\n",
    "        fingers = [thumb_tip, index_finger_tip, middle_finger_tip, ring_finger_tip, little_finger_tip]\n",
    "        extended_fingers = sum([1 for finger in fingers if finger.y < hand_landmarks.landmark[0].y])  # count extended fingers\n",
    "        if extended_fingers >= 4:\n",
    "            return \"Stop\"\n",
    "\n",
    "        # Check for \"I Love You\" gesture (index, little finger, and thumb extended)\n",
    "        \n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "# Initialize the gesture recognition class\n",
    "gesture_recognition = GestureRecognition()\n",
    "\n",
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Detect the gesture\n",
    "    gesture_recognition.detect_gesture(frame)\n",
    "\n",
    "    # Display the detected gesture on the frame\n",
    "    gesture_text = gesture_recognition.current_gesture if gesture_recognition.current_gesture else \"No gesture detected\"\n",
    "    cv2.putText(frame, f\"Gesture: {gesture_text}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "    # Show the frame with the gesture output\n",
    "    cv2.imshow('Gesture Recognition', frame)\n",
    "\n",
    "    # Break the loop if the user presses the 'q' key\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close any open windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e802f3-f98e-4074-9f88-6e396374a996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427947d8-22ee-4b73-b948-515b9eff291d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
